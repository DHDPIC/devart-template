#Performance Implications

The crux of our idea is regarding a confluence or 'conflux' of data provided from multiple users connected to the installation via a web application using web sockets over a node server. This is taken from the geographical term where tributaries join a larger river. Dealing with data from potentially a large number of connected devices introduces several performance issues, particularly when we want to use this data to run real time 3D graphics. These include:

* Data normalization / cleaning
* Websocket bandwidth
* Data updates during animation
* Combining data from multiple sources

The first two of these can be solved relatively simply and that is to do as much data processing on the smartphones as possible, before emitting to the node server. Doing this on the devices at the edge of the system decentralises the work reducing performance bottlenecks. We should also only transmit data we are actually interested in as well as taking care to limit the frequency of these emits to conserve bandwidth over the wire. There is no point sending this data more frequently than frames will be rendered. The accelerometers and gyroscopes in modern smartphones are extremely sensitive, we definitely don't want to simply listen to these events and emit them straight through the websocket! This sensitivity also doesn't look pretty, if the orientation values are plugged straight into a 3D visualisation the results are juddery and unsettling.

To fix both of these issues, firstly we only emit the orientation data from the phone periodically (say 60 times a second to match our max frame rate), in between these emits we can also smooth the data somewhat by storing values in a fixed length array and averaging them out (It's actually a little more complex than this as iPhones and Androids conveniently report orientation slightly differently and you also have to deal with wrapping around rotations from -180 to +180 degrees). If we were to do this centrally for 100 devices, we would be doing a fair amount of computation on the CPU before trying to render the scene in real time, definitely not a good thing. But each device can easily handle this computation on its own.

The third performance issue listed above is how we actually integrate these data updates with the animation. Again we definitely don't want to be updating the scene every time we receive a data update. This is simple in some respects as we just update the data store in memory whenever we receive an event but only render the scene in a requestAnimationFrame callback. But this links in with the last performance point which is combining data from a potentially large number of sources. Especially as we'll be applying a variation of the flocking algorithm as a means to combine this data and create our vertex height map. The flocking algorithm is elegantly simple and a great example of emergent behaviour, but it suffers from a network effect (namely [Metcalfe's law](http://en.wikipedia.org/wiki/Metcalfe's_law) as we have to test every unit in the system against every other unit. The best way to deal with this would be pushing as much computation onto the graphics card as possible as shaders can run in parallel. Enter GPGPU.

Without going into minute details, essentially what we're doing is using a texture as an array, assigning one pixel to each input device. We need to use a special type of texture called a floating point texture, which is relatively new to WebGL. If you take a standard RGBA texture, each channel is stored in an 8 bit unsigned integer, this is fine for an image but pretty useless for maths. A floating point texture as it name suggests allows you to store a float in each channel, which means we can start to move meaningful data to the GPU. We can also create floating point frame buffer objects (FBO), which means we can render to them too. In our project we have two frame buffer objects, one to store each device's current position (x, y, z positions stored in R, G, B channels respectively) and one to store its next position (this is necessary as you can't read and write to a frame buffer at the same time - well technically you probably can but you don't know what you'll end up with). We then have a floating point texture which stores the orientation data received from each device (alpha, beta, gamma stored in R, G, B channels respectively). The simulation itself will actually be run in a fragment shader, there is a bit of set up involved as we need to render a quad with a basic vertex shader in order to get our data into the fragment shader, but it's not too tricky - it's not dissimilar to standard render to texture techniques used in post processing. The fragment shader takes the FBO containing the devices' current positions and the orientation floating point texture as uniform inputs, runs the flocking algorithm and outputs to the second FBO. This second FBO is then used as the input to a second fragment shader which actually generates the height map used to render the sphere. The first and the second FBOs are then swapped (ping-ponging) ready for the next frame.